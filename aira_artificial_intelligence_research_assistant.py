# -*- coding: utf-8 -*-
"""AIRA - Artificial Intelligence Research Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T15xJTf_E_uH-vS2_s7Ryxh88jxCvs2p

# Literature review assistant

The aim of this project is to create an AI-powered literature review assistant called AIRA. This research assistant will use GPT 3.5

# Libraries
"""

!pip install PyPDF2 gdown openai tiktoken pypdf chromadb langchain

from PyPDF2 import PdfReader
import os
import glob
import pandas as pd
import re
import tiktoken

from PIL import Image
from io import BytesIO

# import pytesseract
# Specify the path where Tesseract-OCR was installed
# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
import pandas as pd
import numpy as np
import cv2
import matplotlib.pyplot as plt
# from pytesseract import Output
import re
import glob
import os
import PIL.Image
from PIL import Image
#from pdf2image import convert_from_path

# Chat GPT
import tiktoken
from langchain.text_splitter import RecursiveCharacterTextSplitter
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
import chromadb
from openai import OpenAI

from google.colab import drive
drive.mount('/content/drive')

"""
# Creating the database

We first extract the text from the PDF of our literature review. The final output of this section is a dataframe with two columns, the first one is the name od the PDF, the second one is the extracted text of the"""

def pdf_to_text(pdf_path):
    '''
    Objective:
        This functions transforms a pdf to a text where we can apply text information retrieval

    Input:
        pdf_path (str) : The path where the pdf is located, including the pdf name.

    Output:
        It returns the text of the pdf
    '''
    reader   = PdfReader( pdf_path )
    n_pages  = len( reader.pages )
    print( f'Number of pages: { n_pages }' )

    try:
        extracted_text = [ reader.pages[ i ].extract_text() for i in range( n_pages ) ]
        print( 'Text successfully extracted' )

    except:
        extracted_text = []
        print( 'Text not found' )


    combined_text = '\n'.join( extracted_text )


    return combined_text

def extract_text( combined_text , start_pattern , end_pattern):
    '''
    Objective:
        This function takes a text and extracts the patter indicated by the start_patter and end_patter inputs.

    Input:
        combined_text (str) : The text where we can extract information.

        start_pattern (str) : The starting pattern.

        end_pattern (str) : The ending pattern.
    '''
    start_match   = re.search( start_pattern, combined_text, re.IGNORECASE )
    end_match     = re.search( end_pattern, combined_text[ start_match.end(): ], re.IGNORECASE )

    end_index     = start_match.end() + end_match.start()
    article_text  = combined_text[ start_match.end(): end_index ].strip()

    article_text = article_text.split('\n')

    return article_text

folder_path = '/content/drive/MyDrive/Hackaton/Literature'
data_dict = {'Filename': [], 'PDF_text': []}

# for pattern in patterns:
for filename in os.listdir(folder_path):
    file_path = os.path.join(folder_path, filename)

    try:
        file_pdf = pdf_to_text(file_path)

    except:
        print(f'{filename} is not a file')
        continue

    data_dict['Filename'].append(filename)
    data_dict['PDF_text'].append(file_pdf)

final_dataframe = pd.DataFrame(data_dict)

final_dataframe.iloc[0, 1]

final_dataframe

"""#Chunks and tokenization

We then proceed to divide our information in big chunks because each PDF contains a huge number of token. This will allow GPT 3.5 to read the information without surpassing the limit of tokens
"""

final_dataframe["PDF_text"]

tokenizer = tiktoken.get_encoding("cl100k_base")

# We must separate the text into chunks
def tokenCounter(text):
    return len(tokenizer.encode(text))

def generate_chunks(row):
    # Reducir el tamaño del chunk para generar menos chunks
    textSplitter = RecursiveCharacterTextSplitter(
        chunk_size=6500,  # Reducir el tamaño del chunk aún más
        chunk_overlap=100,  # Reducir el solapamiento del chunk a la mitad
        length_function=tokenCounter,
        separators=["\n\n", ".", "\n", " "]
    )
    chunks = textSplitter.create_documents([row["PDF_text"]], metadatas=[{"id": f"pdf{row.name + 1}"}])
    return f"pdf{row.name + 1}", chunks

# Aplicamos la función generate_chunks a cada fila del dataframe y convertimos los resultados en un diccionario
chunks_dict = dict(final_dataframe.apply(generate_chunks, axis=1).values)

tokenCounter(str(chunks_dict.values()))

OPENAI_API_KEY = "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"

openaiEmbedding = OpenAIEmbeddingFunction(
        api_key=OPENAI_API_KEY,
        model_name="text-embedding-3-small"
)

#  import chromadb

#  chromaClient = chromadb.PersistentClient()
#  collection = chromaClient.create_collection(
#      name="aira_collection_FINAL_FFF",
#      embedding_function=openaiEmbedding,
#      #metadata={"hnsw:space": "cosine"}
#  )

#  collection.add(
#          documents=[document.page_content for document in chunks],
#          #metadatas=[document.metadata for document in chunks],
#          ids=[f"id{i+1}" for i in range(len(chunks))]
#  )

# tokenCounter(str(collection.get()))

"""# Functions and prompts"""

client = OpenAI(api_key = "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")

DELIMITER = "####"

def getCompletionFromMessages(
        query,
        messages,
        model="gpt-4",
        temperature=0,
        delimiter=DELIMITER
):
    query = f"{DELIMITER}{query}{DELIMITER}"
    messages = [{"role": "user", "content": query}]
    response = client.chat.completions.create(
        messages=messages,
        temperature=temperature,
        model=model
    )
    responseContent = response.choices[0].message.content
    messages = [{"content": responseContent, "role": "assistant"}]
    return messages


results = []

for i in range(1, 4):  # Iterar sobre pdf1, pdf2, pdf3, ...
    current_pdf = f"pdf{i}"

    system_prompt = f"""
    You are a literature review assistant. You will be given a dictionary of\
    papers stored in {str(chunks_dict[current_pdf][0])}. You will be asked to analyse the text of each paper  \
    and give a response to a series of questions. You will answer in a \
    professional manner using academic language. Your responses must be concise, \

    Question 1: Title (string)

    Question 2: Author (string)

    Question 3: Journal of publication (string)

    Question 4: Year of publication (numeric)

    Question 5: Keywords after the abstract (string)

    Question 6: Author's institutional affiliation

    Question 7:  DOI (string)

    Use the following JSON format for your response:
    Question 1 : Response 1
    Question 2 : Response 2
    Question N : Response N
    """

    messages = [{"role": "user", "content": system_prompt}]

    completion_result = getCompletionFromMessages(system_prompt, messages)
    results.append(completion_result)

getCompletionFromMessages(system_prompt, messages)

results

data = {}

for i, result in enumerate(results):
    pdf_name = f"pdf{i + 1}"
    content = result[0]["content"]

    # Dividir el contenido en líneas
    lines = content.split('\n')

    # Extraer las respuestas
    for line in lines:
        if ':' in line:
            question, response = map(str.strip, line.split(':', 1))
            # Renombrar las columnas según lo solicitado
            if question == "Question 1":
                question = "Title"
            elif question == "Question 2":
                question = "Author"
            elif question == "Question 3":
                question = "Journal of publication"
            elif question == "Question 4":
                question = "Year of publication"
            elif question == "Question 5":
                question = "Keywords after the abstract"
            elif question == "Question 6":
                question = "JEL codes"
            elif question == "Question 7":
                question = "DOI"

            data.setdefault(question, []).append(response)

data

# Crear el DataFrame
df = pd.DataFrame(data)

# Agregar una columna con los nombres de los PDFs
df['PDF'] = [f'pdf{i + 1}' for i in range(3)]

# Reordenar las columnas para tener 'PDF' al principio
df = df[['PDF'] + [col for col in df.columns if col != 'PDF']]

# Mapeo de nombres
column_mapping = {
    '"Question 1"': "Title",
    '"Question 2"': "Author",
    '"Question 3"': "Journal of publication",
    '"Question 4"': "Year of publication",
    '"Question 5"': "Keywords after the abstract",
    '"Question 6"': "Authors institutional affiliation",
    '"Question 7"': "DOI"
}

# Renombrar las columnas
df.rename(columns=column_mapping, inplace=True)
df
